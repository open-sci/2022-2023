{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#D6D58E\">Lorenzo Paolini - OpenScience Project Notebook</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#9FC131\">Research question</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How many citations (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta? What are the disciplines that cites the most and those cited the most? How many citations start from and go to publications in OpenCitations Meta that are not included in SSH journals?</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#D6D58E\">General abstract - Progressive update</span>\n",
    "#### Last update: Week 1 (20/26 march)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Purpose</b>: we want to find out the following:\n",
    "- by looking at citations data contained in COCI, the number of citations included in Meta which refer to publication in SSH (Social Sciences and Humanities) journals indicated in ERIH-PLUS;\n",
    "- the disciplines citing the most VS the disciplines cited the most;\n",
    "- the citations from/to publication contained in Meta which are not included in SSH journals.\n",
    "\n",
    "We want to create a connection between these three different datasets in order to have an overall view of the citations present in each of them.\n",
    "\n",
    "\n",
    "<b>Methodology</b>: we will approach the problem from a computational point of view, by building a python software able to analyse the data, querying them in order to retrieve the information needed, and to present the results in a clear and understandable way.\n",
    "\n",
    "\n",
    "<b>Findings</b>: for what concerns the findings, up to today, we can't see meaningful differences in the number of citations coming from different disciplines, since it is related to the subject of the study, while the ones cited the most belong to psychology, health and science studies.\n",
    "\n",
    "\n",
    "<b>Originality/Value</b>: our research can be defined as very valuable, since it adds information to existing resources with the aim of facilitating their use and allowing the users to have a clearer view of the data contained in each dataset. Further development will be made. For example, we could analyse other disciplines, to have the same overview as the one created by us but related to other fields.\n",
    "\n",
    "\n",
    "<i><b>Keywords</b>: OpenScience, Citation, OC-COCI, OC-Meta, ERIH-PLUS, journals</i>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 1</span>\n",
    "##### <i>20/03 - 25/03</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week we have defined the abstract for our work. Additionally, I have started to download the data that we will use to carry out our project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 2</span>\n",
    "##### <i>27/03 - 01/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second week, which goes together with the third one (Easter things), I have created my own personal ORCID.</br>\n",
    "Additionally, I have finally downloaded all the data for the final project, and started to explore them in detail. The aim of this exploration was to have a better grasp on what we have at our disposal in order to answer the research questions provided at the top of this notebook.</br>\n",
    "\n",
    "The bigger part of the exploration has been done thanks to pandas and os libraries. I still have some doubts for what concerns COCI in particular. I am not sure about which data should I work on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#D6D58E\">Data Management Plan</span>\n",
    "Together with my group, we have defined the first draft of the data management plan of our project, and deposited it permanently on Zenodo. According to the requests, we have produced it for two datasets:\n",
    "- one for the data we will use for our project, and \n",
    "- another for the software we will develop to analyse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 3</span>\n",
    "##### <i>03/04 - 08/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#D6D58E\">Workflow</span>\n",
    "Together with my group, we have also defined and wrote a first version of our workflow in [protocols.io](https://www.protocols.io/). The workflow is not precisely defined yet, this is due to the fact that we still need to understand better what we aim to do. \n",
    "\n",
    "After an additional review of the workflow, this morning we have obtained a DOI for the first version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 4</span>\n",
    "##### <i>10/04 - 15/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week I have tried to get back the lecture I've missed but I didn't manage to do it all. Nonetheless, I have investigated better the topics about Peer review and did the review to the other group's Data Management Plan, trying to make it as useful as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 5</span>\n",
    "##### <i>17/04 - 22/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed a new release by ERIH-PLUS. We have decided all together to use such version to conduct our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we met several times with the other members of the group in order to revise the DMP and the Protocol according to the reviews we received. New versions of both the research outcomes have been published. Additionally, we discussed and prepared some answers to our reviewers, which will be published and subsequently delivered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another result reahced by means of these meetings has been a first united and commonly agreed version of the final software, which has been reasoned and started to be written. In particular, we decided to re-use part of the code taken from preprocessing operations developed inside OpenCitations, properly linked in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 6</span>\n",
    "##### <i>24/04 - 29/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week, we discussed better the workflow of the project. We have also run some experiments on COCI's preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have started to work in order to provide executable bash files to make the entire process easier to be reproduced. Accordingly, I have created a new branch in our github repository, containing these new files and some tests that needs to be investigated better.\n",
    "\n",
    "For now, I have developed a .sh file useful to automatically download all the original files that will then be processed by an additional .sh file, which has been started to be developed. For now, the preprocessing.sh file contains only a first version of COCI preprocessing operations.\n",
    "\n",
    "I have also written a first version of the README of this new branch, useful to explain how to deal with such files and what commands are needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 7</span>\n",
    "##### <i>01/05 - 06/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewed the original answer to the other group's review of our DMP, according to the double check done with Sara, and send it back to her for publishing it on Zenodo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have started working on the code in order to answer to the three research questions.\n",
    "This work has been done in parallel to the one done by the other members of the group. In this way we should also be able to have different versions capable of solving the problem, but also to double-check with better precision the results that came from our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 8</span>\n",
    "##### <i>08/05 - 13/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the codes are now ready. I have produced the answers to both the first and the third research questions, and we are waiting for a double check with the other members' analysis results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the way in which I have thought at the problems, the best way to deal with such big data is to produce smaller versions of the same data, but with less information. According to this, I have stored the DOI's contained in META, that has a SSH publisher in a new .csv file, called ERIH_META. This file has been produced also by the other members of the group but, since it was not properly working on my machine, I devised a way to produce my own copy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Preprocess erih in json\n",
    "erih_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/Processed_ERIH/erih_preprocessed.csv\"\n",
    "\n",
    "erih = pd.read_csv(erih_dir_path, delimiter=';', encoding='utf-8')\n",
    "erih_dict = {}\n",
    "erih_disciplines = set()\n",
    "for idx, row in tqdm(erih.iterrows()):\n",
    "    erih_dict[row[\"venue_id\"]] = []\n",
    "    disciplines = row[\"ERIH_disciplines\"].split(',')\n",
    "    for discipline in disciplines:\n",
    "        erih_dict[row[\"venue_id\"]].append(discipline.strip())\n",
    "        erih_disciplines.add(discipline.strip())\n",
    "\n",
    "with open(\"erih_dict.json\", \"w\") as f:\n",
    "    json.dump(erih_dict, f)\n",
    "\n",
    "with open(\"erih_disciplines.json\", \"w\") as f:\n",
    "    disciplines = {}\n",
    "    for discipline in erih_disciplines:\n",
    "        disciplines[discipline] = 0\n",
    "    json.dump(disciplines, f)\n",
    "\n",
    "# Build erih-meta in csv files\n",
    "\n",
    "meta_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/Processed_META/\"\n",
    "erih_meta_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_prep/\"\n",
    "meta_filenames = [filename for filename in os.listdir(meta_dir_path) if os.path.isfile(os.path.join(meta_dir_path, filename)) \n",
    "                                                                                and not filename.startswith(\"._\")]\n",
    "\n",
    "for filename in tqdm(meta_filenames):\n",
    "    # read\n",
    "    meta_df = pd.read_csv(os.path.join(meta_dir_path, filename), delimiter=',', encoding='utf-8')\n",
    "    # drop nan from venue column\n",
    "    meta_df = meta_df.dropna(subset=['venue'])\n",
    "    # add a new column\n",
    "    meta_df[\"ERIH_disciplines\"] = \"\"\n",
    "    # iterate over rows\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        # get the venue id\n",
    "        venue_ids = row[\"venue\"].split(' ')\n",
    "        if len(venue_ids) == 1:\n",
    "            venue_id = venue_ids[0]\n",
    "            # check if the venue id is in the erih_dict\n",
    "            if venue_id in erih_dict:\n",
    "                # get the disciplines\n",
    "                disciplines = erih_dict[venue_id]\n",
    "                # append the disciplines to the row\n",
    "                meta_df.at[idx, \"ERIH_disciplines\"] = disciplines\n",
    "        else:\n",
    "            for venue_id in venue_ids:\n",
    "                # check if the venue id is in the erih_dict\n",
    "                if venue_id in erih_dict:\n",
    "                    # get the disciplines\n",
    "                    disciplines = erih_dict[venue_id]\n",
    "                    # append the disciplines to the row\n",
    "                    meta_df.at[idx, \"ERIH_disciplines\"] = disciplines\n",
    "                    break\n",
    "    # save the dataframe -> one by one...\n",
    "    meta_df.to_csv(os.path.join(erih_meta_dir_path, filename), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I have divided each paper in the newly built ERIH-META in order to have a clear view of SSH and NOT_SSH publications. This has been saved in a JSON, as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter each erih_meta csv in order to divide ssh and not_ssh dois in json\n",
    "\n",
    "erih_meta_filenames = [filename for filename in os.listdir(erih_meta_dir_path) if os.path.isfile(os.path.join(erih_meta_dir_path, filename)) \n",
    "                                                                                and not filename.startswith(\"._\")]\n",
    "\n",
    "\n",
    "erih_meta_papers = {'ssh_papers':list(),\n",
    "                    'not_ssh_papers':list()}\n",
    "\n",
    "for filename in tqdm(erih_meta_filenames):\n",
    "    df = pd.read_csv(os.path.join(erih_meta_dir_path, filename))\n",
    "    df = df[['id', 'ERIH_disciplines']]\n",
    "    # fill all the possible NaN or None with \"\"\n",
    "    df = df.fillna('')\n",
    "    # create boolean mask for erih_disciplines column\n",
    "    mask = df['ERIH_disciplines'] != ''\n",
    "\n",
    "    # filter the dataframe with this mask\n",
    "    ssh_df = df[mask]\n",
    "    ssh_df = ssh_df.reset_index(drop=True)\n",
    "\n",
    "    # Create a second dataframe from the above mask, where are kept only the False rows in the mask\n",
    "    not_ssh_df = df[~mask]\n",
    "    not_ssh_df = not_ssh_df.reset_index(drop=True)\n",
    "\n",
    "    # Get the unique values of the id column\n",
    "    unique_ssh = ssh_df['id'].unique().tolist()\n",
    "    unique_not_ssh = not_ssh_df['id'].unique().tolist()\n",
    "\n",
    "    # Append the unique values to the list\n",
    "    erih_meta_papers['ssh_papers'].extend(unique_ssh)\n",
    "    erih_meta_papers['not_ssh_papers'].extend(unique_not_ssh)\n",
    "\n",
    "# Save inside JSON\n",
    "with open(\"erih_meta_papers.json\", \"w\") as f:\n",
    "    json.dump(erih_meta_papers, f)\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above reults, I have looked whether there were (as can be spotted in the .csv of meta) more DOIs for the same work, I have divided them, and then I have removed possible double values, and created a new dictionary of unique DOIs for SSH and NOT_SSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean double dois in erih-meta\n",
    "\n",
    "#load erih json into dict\n",
    "with open(\"erih_meta_papers.json\", \"r\") as f:\n",
    "    erih_meta_papers = json.load(f)\n",
    "\n",
    "ssh_papers = []\n",
    "for ssh_pap in tqdm(erih_meta_papers['ssh_papers']):\n",
    "    papers = ssh_pap.split(' ')\n",
    "    ssh_papers.extend(papers)\n",
    "\n",
    "not_ssh_papers = []\n",
    "for not_ssh_pap in tqdm(erih_meta_papers['not_ssh_papers']):\n",
    "    papers = not_ssh_pap.split(' ')\n",
    "    not_ssh_papers.extend(papers)\n",
    "\n",
    "# Create a new dict with the unique values\n",
    "erih_meta_papers_unique = {'ssh_papers':list(set(ssh_papers)),\n",
    "                            'not_ssh_papers':list(set(not_ssh_papers))}\n",
    "\n",
    "with open(\"erih_meta_papers_unique.json\", \"w\") as f:\n",
    "    json.dump(erih_meta_papers_unique, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have developed this snippet to answer to both Q1 and Q2 that is still pretty slow. It still needs to be optimized (and to run in parallel cores), I will do it if the answers to the two research questions are correct and if we decide with the other members of the group to use this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "coci_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/smaller_COCI/\"\n",
    "\n",
    "# Scan directory\n",
    "coci_filenames = []\n",
    "with os.scandir(coci_dir_path) as entries:\n",
    "    for entry in tqdm(entries, desc='Iterating filenames...', colour='blue', smoothing=0.1, total=len(os.listdir(coci_dir_path))):\n",
    "        if entry.is_file() and not entry.name.startswith(\"._\"):\n",
    "            coci_filenames.append(entry.name)\n",
    "\n",
    "print('Reading erih-meta...')\n",
    "with open(\"erih_meta_papers_unique.json\", \"r\") as f:\n",
    "    erih_meta_papers_unique = json.load(f)\n",
    "\n",
    "print('Building sets...')\n",
    "ssh_set = set(erih_meta_papers_unique['ssh_papers'])\n",
    "not_ssh_set = set(erih_meta_papers_unique['not_ssh_papers'])\n",
    "\n",
    "ssh_citations = 0\n",
    "not_ssh_citations = 0\n",
    "\n",
    "def count_citations(row):\n",
    "    \"\"\"\n",
    "    This function is used thanks to the apply method of pandas.\n",
    "    \"\"\"\n",
    "    # The row contains an SSH citation? -> This is with an OR\n",
    "    if row['citing'] in ssh_set or row['cited'] in ssh_set:\n",
    "        return 'ssh'\n",
    "    # The row contains a non-SSH citation? -> This is with an AND\n",
    "    elif row['citing'] in not_ssh_set and row['cited'] in not_ssh_set:\n",
    "        return 'not_ssh'\n",
    "    # If not inside\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "for filename in tqdm(coci_filenames, desc='Iterating files...', colour='green', smoothing=0.1):\n",
    "    df = pd.read_csv(os.path.join(coci_dir_path, filename))\n",
    "\n",
    "    # Apply count_citations to each row of the DF\n",
    "    citation_counts = df.apply(count_citations, axis=1).value_counts()\n",
    "    # Increment the SSH citation count and non-SSH citation count\n",
    "    ssh_citations += citation_counts.get('ssh', 0)\n",
    "    not_ssh_citations += citation_counts.get('not_ssh', 0)\n",
    "\n",
    "print(ssh_citations)\n",
    "print(not_ssh_citations)   \n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the results I got from it:\n",
    "- SSH Count in META: 225370804\n",
    "- NOT_SSH Count in META: 985223927"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the numbers are the same, thus both methods work. I am currently waiting for the other members of the group to run their method in different computer architectures, then we will se which one is faster and which has more possibilities to be optimized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 9</span>\n",
    "##### <i>15/05 - 19/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week we have double-checked our results, and we have joined all the methods inside a single class. I have extended and tried to optimize (by merging together the various methods and functions) the code written by the other members of the group, as well as my code. The final result is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from lib.csv_manager_erih_meta_disciplines import CSVManager\n",
    "\n",
    "class Counter(object):\n",
    "    _entity_columns_to_use_erih_meta_disciplines = ['id', 'erih_disciplines']\n",
    "    _entity_columns_to_use_erih_meta_without_disciplines = ['id']\n",
    "    _entity_columns_to_use_q1_q3 = ['citing', 'cited']\n",
    "    _entity_columns_to_use_q2 = ['id', 'citing', 'cited', 'disciplines']\n",
    "\n",
    "    def __init__(self, coci_preprocessed_path, erih_meta_path):\n",
    "        self._list_coci_files = self.get_all_files(coci_preprocessed_path, '.csv')\n",
    "        self._list_erih_meta_files = self.get_all_files(erih_meta_path, '.csv')\n",
    "        self.num_cpu = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def get_all_files(self, i_dir_or_compr, req_type):\n",
    "        '''It returns a list containing all the files found in the input folder and with the extension required, like \".csv\".'''\n",
    "        result = []\n",
    "        if os.path.isdir(i_dir_or_compr):\n",
    "            for cur_dir, cur_subdir, cur_files in os.walk(i_dir_or_compr):\n",
    "                for cur_file in cur_files:\n",
    "                    if cur_file.endswith(req_type) and not os.path.basename(cur_file).startswith(\".\"):\n",
    "                        result.append(os.path.join(cur_dir, cur_file))\n",
    "        return result\n",
    "\n",
    "    def splitted_to_file(self, cur_n, lines, columns_to_use, output_dir_path):\n",
    "        '''\n",
    "        This method is responsible for writing the new csv files, with the columns passed as input.\n",
    "        It concretely produces output files by creating in the output folder a new file every n lines\n",
    "        which come from the other methods (like \"create_erih_meta_with_disciplines\", \"create_dataset_SSH\", etc.)\n",
    "        where n is the integer number defined as an input parameter.\n",
    "        In particular, the method takes in input the current number of lines, a data structure containing\n",
    "        the lines to write in the output file, the name of the columns of the new csv files, the path of the directory to store the new files.\n",
    "        '''\n",
    "        if int(cur_n) != 0 and int(cur_n) % int(self._interval) == 0:\n",
    "            filename = \"count_\" + str(cur_n // self._interval) + '.csv'\n",
    "            if os.path.exists(os.path.join(output_dir_path, filename)):\n",
    "                cur_datetime = datetime.now()\n",
    "                dt_string = cur_datetime.strftime(\"%d%m%Y_%H%M%S\")\n",
    "                filename = filename[:-len('.csv')] + \"_\" + dt_string + '.csv'\n",
    "            with open(os.path.join(output_dir_path, filename), \"w\", encoding=\"utf8\", newline=\"\") as f_out:\n",
    "                dict_writer = csv.DictWriter(f_out, delimiter=\",\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\",\n",
    "                                             fieldnames=columns_to_use)\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows(lines)\n",
    "                f_out.close()\n",
    "            lines = []\n",
    "            return lines\n",
    "        else:\n",
    "            return lines\n",
    "\n",
    "    #def create_erih_meta_with_disciplines(self):\n",
    "    #    '''This method, starting from the \"ERIH_META\" dataset creates a subset of it, containing just the ids with at least a discipline associated.\n",
    "    #    It has two columns: 'id' and 'erih_disciplines' '''\n",
    "    #    output_erih_meta_disciplines = os.path.join(self._output_dir + 'erih_meta_with_disciplines')\n",
    "    #    if not exists(output_erih_meta_disciplines):\n",
    "    #        os.makedirs(output_erih_meta_disciplines)\n",
    "    #    data = []\n",
    "    #    count = 0\n",
    "    #    for file_idx, file in enumerate(tqdm(self._list_erih_meta_files), 1):\n",
    "    #        chunksize = 10000\n",
    "    #        with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "    #            for chunk in reader:\n",
    "    #                chunk.fillna(\"\", inplace=True)\n",
    "    #                df_dict_list = chunk.to_dict(\"records\")\n",
    "    #                for line in df_dict_list:\n",
    "    #                    discipline = line.get('erih_disciplines')\n",
    "    #                    if discipline:\n",
    "    #                        data.append(line)\n",
    "    #                        count += 1\n",
    "    #                        if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "    #                            data = self.splitted_to_file(count, data, self._entity_columns_to_use_erih_meta_disciplines, output_erih_meta_disciplines)\n",
    "    #    if len(data) > 0:\n",
    "    #        count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "    #        self.splitted_to_file(count, data, self._entity_columns_to_use_erih_meta_disciplines, output_erih_meta_disciplines)\n",
    "\n",
    "    #def create_erih_meta_without_disciplines(self):\n",
    "    #    '''This method, starting from the \"ERIH_META\" dataset creates a subset of it, containing just the ids without a discipline associated.\n",
    "    #    It has just one column: 'id' '''\n",
    "    #    output_erih_meta_without_disciplines = os.path.join(self._output_dir + 'erih_meta_without_disciplines')\n",
    "    #    if not exists(output_erih_meta_without_disciplines):\n",
    "    #        os.makedirs(output_erih_meta_without_disciplines)\n",
    "    #    data = []\n",
    "    #    count = 0\n",
    "    #    for file_idx, file in enumerate(tqdm(self._list_erih_meta_files), 1):\n",
    "    #        chunksize = 10000\n",
    "    #        with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "    #            for chunk in reader:\n",
    "    #                chunk.fillna(\"\", inplace=True)\n",
    "    #                df_dict_list = chunk.to_dict(\"records\")\n",
    "    #                for line in df_dict_list:\n",
    "    #                    new_line = dict()\n",
    "    #                    discipline = line.get('erih_disciplines')\n",
    "    #                    if not discipline:\n",
    "    #                        new_line['id'] = line.get('id')\n",
    "    #                        data.append(new_line)\n",
    "    #                        count += 1\n",
    "    #                        if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "    #                            data = self.splitted_to_file(count, data, self._entity_columns_to_use_erih_meta_without_disciplines, output_erih_meta_without_disciplines)\n",
    "    #    if len(data) > 0:\n",
    "    #        count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "    #        self.splitted_to_file(count, data, self._entity_columns_to_use_erih_meta_without_disciplines, output_erih_meta_without_disciplines)\n",
    "\n",
    "    def create_additional_files(self, with_disciplines):\n",
    "        process_output_dir = self._path_erih_meta_with_disciplines if with_disciplines else self._path_erih_meta_without_disciplines\n",
    "        entity_columns_to_use = self._entity_columns_to_use_erih_meta_disciplines if with_disciplines else self._entity_columns_to_use_erih_meta_without_disciplines\n",
    "        \n",
    "        if not exists(process_output_dir):\n",
    "            os.makedirs(process_output_dir)\n",
    "\n",
    "        data = []\n",
    "        count = 0\n",
    "\n",
    "        if with_disciplines:\n",
    "            for _, file in enumerate(tqdm(self._list_erih_meta_files), 1):\n",
    "                chunksize = 10000\n",
    "                with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "                    for chunk in reader:\n",
    "                        chunk.fillna(\"\", inplace=True)\n",
    "                        df_dict_list = chunk.to_dict(\"records\")\n",
    "                        for line in df_dict_list:\n",
    "                            discipline = line.get('erih_disciplines')\n",
    "                            if discipline:\n",
    "                                data.append(line)\n",
    "                                count += 1\n",
    "                                if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                    data = self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)\n",
    "        else:\n",
    "            for _, file in enumerate(tqdm(self._list_erih_meta_files), 1):\n",
    "                chunksize = 10000\n",
    "                with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "                    for chunk in reader:\n",
    "                        chunk.fillna(\"\", inplace=True)\n",
    "                        df_dict_list = chunk.to_dict(\"records\")\n",
    "                        for line in df_dict_list:\n",
    "                            discipline = line.get('erih_disciplines')\n",
    "                            new_line = dict()\n",
    "                            if not discipline:\n",
    "                                new_line['id'] = line.get('id')\n",
    "                                data.append(new_line)\n",
    "                                count += 1\n",
    "                                if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                    data = self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)\n",
    "            \n",
    "        if len(data) > 0:\n",
    "            count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "            self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)    \n",
    "\n",
    "#####################\n",
    "\n",
    "    #def create_dataset_SSH(self):\n",
    "    #    '''This method creates, starting from \"COCI_preprocessed\", the dataset that we use for answering to the first research question.\n",
    "    #    It has two columns 'citing' and 'cited', and contains just the DOIs that belongs to SSH journals.'''\n",
    "    #    output_q1 = os.path.join(self._output_dir + 'dataset_SSH')\n",
    "    #    if not exists(output_q1):\n",
    "    #        os.makedirs(output_q1)\n",
    "    #    self._CSVManager_erih_meta_with_disciplines = CSVManager(self._path_erih_meta_with_disciplines)\n",
    "    #    data = []\n",
    "    #    count = 0\n",
    "    #    for file_idx, file in enumerate(tqdm(self._list_coci_files), 1):\n",
    "    #        chunksize = 10000\n",
    "    #        with pd.read_csv(file, chunksize=chunksize, sep=\",\") as reader:\n",
    "    #            for chunk in reader:\n",
    "    #                chunk.fillna(\"\", inplace=True)\n",
    "    #                df_dict_list = chunk.to_dict(\"records\")\n",
    "    #                for line in df_dict_list:\n",
    "    #                    citing = line.get('citing')\n",
    "    #                    cited = line.get('cited')\n",
    "    #                    if self._CSVManager_erih_meta_with_disciplines.get_value(citing) or self._CSVManager_erih_meta_with_disciplines.get_value(cited):\n",
    "    #                        count += 1\n",
    "    #                        if self._CSVManager_erih_meta_with_disciplines.get_value(citing) and self._CSVManager_erih_meta_with_disciplines.get_value(cited):\n",
    "    #                            data.append(line)\n",
    "    #                        elif self._CSVManager_erih_meta_with_disciplines.get_value(citing):\n",
    "    #                            entity_dict1 = dict()\n",
    "    #                            entity_dict1['citing'] = citing\n",
    "    #                            entity_dict1['cited'] = \"\"\n",
    "    #                            data.append(entity_dict1)\n",
    "    #                        elif self._CSVManager_erih_meta_with_disciplines.get_value(cited):\n",
    "    #                            entity_dict2 = dict()\n",
    "    #                            entity_dict2['cited'] = cited\n",
    "    #                            entity_dict2['citing'] = \"\"\n",
    "    #                            data.append(entity_dict2)\n",
    "#\n",
    "    #                        if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "    #                            data = self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_q1)\n",
    "#\n",
    "    #    if len(data) > 0:\n",
    "    #        count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "    #        self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_q1)\n",
    "#\n",
    "    #def create_dataset_no_SSH(self):\n",
    "    #    '''This method creates, starting from \"COCI_preprocessed\", the dataset that we use for answering to the third research question.\n",
    "    #    It has two columns 'citing' and 'cited', and contains just the DOIs that don't belong to SSH journals.'''\n",
    "    #    output_q3 = os.path.join(self._output_dir + 'dataset_no_SSH')\n",
    "    #    if not exists(output_q3):\n",
    "    #        os.makedirs(output_q3)\n",
    "    #    self._set_erih_meta_without_disciplines = CSVManager.load_csv_column_as_set(self._path_erih_meta_without_disciplines, 'id')\n",
    "    #    data = []\n",
    "    #    count = 0\n",
    "    #    for file_idx, file in enumerate(tqdm(self._list_coci_files), 1):\n",
    "    #        chunksize = 10000\n",
    "    #        with pd.read_csv(file, chunksize=chunksize, sep=\",\") as reader:\n",
    "    #            for chunk in reader:\n",
    "    #                chunk.fillna(\"\", inplace=True)\n",
    "    #                df_dict_list = chunk.to_dict(\"records\")\n",
    "    #                for line in df_dict_list:\n",
    "    #                    citing = line.get('citing')\n",
    "    #                    cited = line.get('cited')\n",
    "    #                    if citing in self._set_erih_meta_without_disciplines or cited in self._set_erih_meta_without_disciplines:\n",
    "    #                        count += 1\n",
    "    #                        if citing in self._set_erih_meta_without_disciplines and cited in self._set_erih_meta_without_disciplines:\n",
    "    #                            data.append(line)\n",
    "    #                        elif citing in self._set_erih_meta_without_disciplines:\n",
    "    #                            entity_dict1 = dict()\n",
    "    #                            entity_dict1['citing'] = citing\n",
    "    #                            entity_dict1['cited'] = \"\"\n",
    "    #                            data.append(entity_dict1)\n",
    "    #                        elif cited in self._set_erih_meta_without_disciplines:\n",
    "    #                            entity_dict2 = dict()\n",
    "    #                            entity_dict2['cited'] = cited\n",
    "    #                            entity_dict2['citing'] = \"\"\n",
    "    #                            data.append(entity_dict2)\n",
    "    #                        if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "    #                            data = self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_q3)\n",
    "    #    if len(data) > 0:\n",
    "    #        count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "    #        self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_q3)\n",
    "\n",
    "    def create_datasets_for_count(self, is_SSH=True):\n",
    "        output_process_dir = self._path_dataset_SSH if is_SSH else self._path_dataset_no_SSH\n",
    "        \n",
    "        if not exists(output_process_dir):\n",
    "            os.makedirs(output_process_dir)\n",
    "\n",
    "        if is_SSH:\n",
    "            self._CSVManager_erih_meta_with_disciplines = CSVManager(self._path_erih_meta_with_disciplines)\n",
    "        else:\n",
    "            self._set_erih_meta_without_disciplines = CSVManager.load_csv_column_as_set(self._path_erih_meta_without_disciplines, 'id')\n",
    "\n",
    "        data = []\n",
    "        count = 0\n",
    "        for _, file in enumerate(tqdm(self._list_coci_files), 1):\n",
    "            chunksize = 10000\n",
    "            with pd.read_csv(file, chunksize=chunksize, sep=\",\") as reader:\n",
    "                for chunk in reader:\n",
    "                    chunk.fillna(\"\", inplace=True)\n",
    "                    df_dict_list = chunk.to_dict(\"records\")\n",
    "                    for line in df_dict_list:\n",
    "                        citing = line.get('citing')\n",
    "                        cited = line.get('cited')\n",
    "\n",
    "                        if is_SSH:\n",
    "                            condition = self._CSVManager_erih_meta_with_disciplines.get_value(citing) or self._CSVManager_erih_meta_with_disciplines.get_value(cited)\n",
    "                        else:\n",
    "                            condition = citing in self._set_erih_meta_without_disciplines and cited in self._set_erih_meta_without_disciplines\n",
    "\n",
    "                        if condition:\n",
    "                            count += 1\n",
    "                            if is_SSH:\n",
    "                                entity_condition1 = self._CSVManager_erih_meta_with_disciplines.get_value(citing)\n",
    "                                entity_condition2 = self._CSVManager_erih_meta_with_disciplines.get_value(cited)\n",
    "                            else:\n",
    "                                entity_condition1 = citing in self._set_erih_meta_without_disciplines\n",
    "                                entity_condition2 = cited in self._set_erih_meta_without_disciplines\n",
    "\n",
    "                            if entity_condition1 and entity_condition2:\n",
    "                                data.append(line)\n",
    "                            elif entity_condition1:\n",
    "                                entity_dict1 = {'citing': citing, 'cited': \"\"}\n",
    "                                data.append(entity_dict1)\n",
    "                            elif entity_condition2:\n",
    "                                entity_dict2 = {'cited': cited, 'citing': \"\"}\n",
    "                                data.append(entity_dict2)\n",
    "\n",
    "                            if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                data = self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_process_dir)\n",
    "\n",
    "        if len(data) > 0:\n",
    "            count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "            self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_process_dir)\n",
    "\n",
    "    def count_lines(self, path):\n",
    "        '''This method simply counts and sums the lines of csv files contained in the folder, the path of which is passed as input'''\n",
    "        citations_count = 0\n",
    "        for file in tqdm(self.get_all_files(path, '.csv')):\n",
    "            results = pd.read_csv(file, sep=\",\")\n",
    "            citations_count += len(results)\n",
    "        return citations_count\n",
    "\n",
    "    def execute_count(self, output_dir='OutputFiles/', create_subfiles=False, interval=10000):\n",
    "        if create_subfiles:\n",
    "            self._interval = interval\n",
    "            self._output_dir = output_dir\n",
    "            if not exists(self._output_dir):\n",
    "                os.makedirs(self._output_dir)\n",
    "            self._path_erih_meta_with_disciplines = os.path.join(output_dir, 'erih_meta_with_disciplines')\n",
    "            self._path_dataset_SSH = os.path.join(output_dir, 'dataset_SSH')\n",
    "            self._path_erih_meta_without_disciplines = os.path.join(output_dir, 'erih_meta_without_disciplines')\n",
    "            self._path_dataset_no_SSH = os.path.join(output_dir, 'dataset_no_SSH')\n",
    "\n",
    "        '''question_1\n",
    "        chiama create_erih_meta_with_disciplines -> crea un dataset con colonne \"id\" e \"erih_disciplines\" contenenti solo i doi SSH con discipline associate \n",
    "        chiama create_dataset_SSH -> crea il dataset che utilizziamo per rispondere alla Q1, cioè un dataset a due colonne \"citing\" e \"cited\" compilato con i doi solo SSH\n",
    "        chiama count_lines -> Va messo come input del metodo \"self._path_dataset_SSH\"; conta le righe del dataset crato con \"create_dataset_SSH e da la risposta alla Q1\"\n",
    "        '''\n",
    "        '''question 3\n",
    "        chiama create_erih_meta_without_disciplines -> crea un dataset a una sola colonna \"id\" che contiene solo i doi senza disciplina associata\n",
    "        chiama create_dataset_no_SSH -> crea il dataset che utilizziamo per rispondere alla Q3, cioè un dataset a due colonne \"citing\" e \"cited\" compilato con i doi non SSH\n",
    "        chiama count_lines -> Va messo come input del metodo \"self._path_dataset_no_SSH\"; conta le righe del dataset crato con \"create_dataset_no_SSH e da la risposta alla Q3\"\n",
    "        '''\n",
    "\n",
    "        if create_subfiles:\n",
    "            # Answer to question 1\n",
    "            self.create_additional_files(with_disciplines=True)\n",
    "            self.create_datasets_for_count(is_SSH=True)\n",
    "            ssh_citations = self.count_lines(self._path_dataset_SSH)\n",
    "            print('Number of citations that (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta: %d' %ssh_citations)\n",
    "\n",
    "            # Answer to question 3\n",
    "            self.create_additional_files(with_disciplines=False)\n",
    "            self.create_datasets_for_count(is_SSH=False)\n",
    "            not_ssh_citations = self.count_lines(self._path_dataset_no_SSH)\n",
    "            print('Number of citations that (according to COCI) start from and go to publications in OpenCitations Meta that are not included in SSH journals: %d' %not_ssh_citations)\n",
    "        else:\n",
    "            print('\\nSarting the process, be patient, it will take a while...\\n')\n",
    "            ssh_papers = list()\n",
    "            not_ssh_papers = list()\n",
    "\n",
    "            for filename in tqdm(self._list_erih_meta_files ,total=len(self._list_erih_meta_files), desc='Building lists of DOIs over ERIH-PLUS and META...', colour='yellow', smoothing=0.1):\n",
    "                df = pd.read_csv(filename) # it was -> os.path.join(erih_meta_dir_path, filename))\n",
    "                df = df[['id', 'erih_disciplines']] # Attention to the name given to the ERIH_disciplines column, if erih or ERIH\n",
    "                # fill all the possible NaN or None with \"\"\n",
    "                df = df.fillna('')\n",
    "                # create boolean mask for erih_disciplines column\n",
    "                mask = df['erih_disciplines'] != ''\n",
    "\n",
    "                # filter the dataframe with the above mask\n",
    "                ssh_df = df[mask]\n",
    "                ssh_df = ssh_df.reset_index(drop=True)\n",
    "\n",
    "                # Create a second dataframe from the above mask, where are kept only the False rows in the mask\n",
    "                not_ssh_df = df[~mask]\n",
    "                not_ssh_df = not_ssh_df.reset_index(drop=True)\n",
    "\n",
    "                # Get the unique values of the id column\n",
    "                unique_ssh = ssh_df['id'].unique().tolist()\n",
    "                unique_not_ssh = not_ssh_df['id'].unique().tolist()\n",
    "\n",
    "                # Append the unique values to the list\n",
    "                ssh_papers.extend(unique_ssh)\n",
    "                not_ssh_papers.extend(unique_not_ssh)\n",
    "\n",
    "            print('Decoupling DOIs from lists...')\n",
    "            ssh_papers_unique = []\n",
    "            for paper in ssh_papers:\n",
    "                papers = paper.split(' ')\n",
    "                ssh_papers_unique.extend(papers)\n",
    "\n",
    "            not_ssh_papers_unique = []\n",
    "            for paper in not_ssh_papers:\n",
    "                papers = paper.split(' ')\n",
    "                not_ssh_papers_unique.extend(papers)\n",
    "\n",
    "            print('Creating sets for unique DOIs...')\n",
    "            ssh_set = set(ssh_papers_unique)\n",
    "            not_ssh_set = set(not_ssh_papers_unique)\n",
    "\n",
    "            ssh_citations = 0\n",
    "            not_ssh_citations = 0\n",
    "\n",
    "            def count_citations(ssh_set, not_ssh_set, row):\n",
    "                if row['citing'] in ssh_set or row['cited'] in ssh_set:\n",
    "                    return 'ssh'\n",
    "                elif row['citing'] in not_ssh_set and row['cited'] in not_ssh_set:\n",
    "                    return 'not_ssh'\n",
    "                else:\n",
    "                    return 'other'\n",
    "            \n",
    "            def count_citations_in_file(ssh_set, not_ssh_set, filepath):\n",
    "                df = pd.read_csv(filepath, usecols=['citing', 'cited'])\n",
    "                citation_counts = df.apply(lambda row: count_citations(ssh_set, not_ssh_set, row), axis=1).value_counts()\n",
    "                return citation_counts.get('ssh', 0), citation_counts.get('not_ssh', 0)\n",
    "\n",
    "            print('Starting to count...\\n')\n",
    "            with ThreadPoolExecutor(max_workers=self.num_cpu) as executor:\n",
    "                count_citations_partial = partial(count_citations_in_file, ssh_set, not_ssh_set)\n",
    "                results = list(tqdm(executor.map(count_citations_partial, self._list_coci_files), total=len(self._list_coci_files), desc='Iterating files...', colour='green', smoothing=0.1))\n",
    "\n",
    "            print('Updating results...')\n",
    "            for ssh_count, not_ssh_count in results:\n",
    "                ssh_citations += ssh_count\n",
    "                not_ssh_citations += not_ssh_count\n",
    "\n",
    "            print('Number of citations that (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta: %d' %ssh_citations)\n",
    "            print('Number of citations that (according to COCI) start from and go to publications in OpenCitations Meta that are not included in SSH journals: %d' %not_ssh_citations)\n",
    "        \n",
    "        print('Done...')\n",
    "        return ssh_citations, not_ssh_citations\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "c = Counter(\"/Volumes/Extreme SSD/OS_data/Processed_data/smaller_COCI/\", \"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_prep/\")\n",
    "count = c.execute_count()\n",
    "print(count)\n",
    "\n",
    "\n",
    "c = Counter(processed_coci_path, erih_meta_path)\n",
    "count = c.execute_count(output_dir=DOVE_PREFERITE, create_subfiles=True, interval=10000)\n",
    "print(count)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 10</span>\n",
    "##### <i>22/05 - 26/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week we started discussing about the best visualizations for the workshop, as well as checked what is left to be done.</br>\n",
    "I have devised a different way to answer the second research question and added it to the previous code.\n",
    "\n",
    "Right now, the code is still running, so in the end we will get a meaningful comparison between the two results.\n",
    "\n",
    "Above the temporary code, to be added inside the new counter class that the other members of the group have developed in order to face some issues discovered during the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import time\n",
    "from lib.csv_manager_erih_meta_disciplines import CSVManager\n",
    "\n",
    "class Counter(object):\n",
    "    _entity_columns_to_use_erih_meta_disciplines = ['id', 'erih_disciplines']\n",
    "    _entity_columns_to_use_erih_meta_without_disciplines = ['id']\n",
    "    _entity_columns_to_use_q1_q3 = ['citing', 'cited']\n",
    "    _entity_columns_to_use_q2 = ['id', 'citing', 'cited', 'disciplines']\n",
    "\n",
    "    def __init__(self, coci_preprocessed_path, erih_meta_path, num_cpus=None):\n",
    "        self._list_coci_files = self.get_all_files(coci_preprocessed_path, '.csv')\n",
    "        self._list_erih_meta_files = self.get_all_files(erih_meta_path, '.csv')\n",
    "        self.num_cpu = num_cpus if num_cpus!=None else multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def get_all_files(self, i_dir_or_compr, req_type):\n",
    "        '''It returns a list containing all the files found in the input folder and with the extension required, like \".csv\".'''\n",
    "        result = []\n",
    "        if os.path.isdir(i_dir_or_compr):\n",
    "            for cur_dir, cur_subdir, cur_files in os.walk(i_dir_or_compr):\n",
    "                for cur_file in cur_files:\n",
    "                    if cur_file.endswith(req_type) and not os.path.basename(cur_file).startswith(\".\"):\n",
    "                        result.append(os.path.join(cur_dir, cur_file))\n",
    "        return result\n",
    "\n",
    "    def splitted_to_file(self, cur_n, lines, columns_to_use, output_dir_path):\n",
    "        '''\n",
    "        This method is responsible for writing the new csv files, with the columns passed as input.\n",
    "        It concretely produces output files by creating in the output folder a new file every n lines\n",
    "        which come from the other methods (like \"create_erih_meta_with_disciplines\", \"create_dataset_SSH\", etc.)\n",
    "        where n is the integer number defined as an input parameter.\n",
    "        In particular, the method takes in input the current number of lines, a data structure containing\n",
    "        the lines to write in the output file, the name of the columns of the new csv files, the path of the directory to store the new files.\n",
    "        '''\n",
    "        if int(cur_n) != 0 and int(cur_n) % int(self._interval) == 0:\n",
    "            filename = \"count_\" + str(cur_n // self._interval) + '.csv'\n",
    "\n",
    "            if os.path.exists(os.path.join(output_dir_path, filename)):\n",
    "                cur_datetime = datetime.now()\n",
    "                dt_string = cur_datetime.strftime(\"%d%m%Y_%H%M%S\")\n",
    "                filename = filename[:-len('.csv')] + \"_\" + dt_string + '.csv'\n",
    "\n",
    "            with open(os.path.join(output_dir_path, filename), \"w\", encoding=\"utf8\", newline=\"\") as f_out:\n",
    "                dict_writer = csv.DictWriter(f_out, delimiter=\",\", quoting=csv.QUOTE_ALL, escapechar=\"\\\\\",\n",
    "                                             fieldnames=columns_to_use)\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows(lines)\n",
    "                f_out.close()\n",
    "\n",
    "            lines = []\n",
    "            return lines\n",
    "        else:\n",
    "            return lines\n",
    "\n",
    "    def create_additional_files(self, with_disciplines):\n",
    "        process_output_dir = self._path_erih_meta_with_disciplines if with_disciplines else self._path_erih_meta_without_disciplines\n",
    "        entity_columns_to_use = self._entity_columns_to_use_erih_meta_disciplines if with_disciplines else self._entity_columns_to_use_erih_meta_without_disciplines\n",
    "        \n",
    "        if not exists(process_output_dir):\n",
    "            os.makedirs(process_output_dir)\n",
    "\n",
    "        data = []\n",
    "        count = 0\n",
    "\n",
    "        if with_disciplines:\n",
    "            for file in tqdm(self._list_erih_meta_files, desc='Processing ERIH-META files to extract SSH DOIs...', total=len(self._list_erih_meta_files), colour='yellow', smoothing=0.1):\n",
    "                chunksize = 10000\n",
    "                with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "                    for chunk in reader:\n",
    "                        chunk.fillna(\"\", inplace=True)\n",
    "                        df_dict_list = chunk.to_dict(\"records\")\n",
    "                        for line in df_dict_list:\n",
    "                            discipline = line.get('erih_disciplines')\n",
    "                            if discipline:\n",
    "                                data.append(line)\n",
    "                                count += 1\n",
    "                                if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                    data = self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)\n",
    "        else:\n",
    "            for file in tqdm(self._list_erih_meta_files, desc='Processing ERIH-META files to extract non-SSH DOIs...', total=len(self._list_erih_meta_files), colour='yellow', smoothing=0.1):\n",
    "                chunksize = 10000\n",
    "                with pd.read_csv(file, usecols=['id', 'erih_disciplines'], chunksize=chunksize, sep=\",\") as reader:\n",
    "                    for chunk in reader:\n",
    "                        chunk.fillna(\"\", inplace=True)\n",
    "                        df_dict_list = chunk.to_dict(\"records\")\n",
    "                        for line in df_dict_list:\n",
    "                            discipline = line.get('erih_disciplines')\n",
    "                            new_line = dict()\n",
    "                            if not discipline:\n",
    "                                new_line['id'] = line.get('id')\n",
    "                                data.append(new_line)\n",
    "                                count += 1\n",
    "                                if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                    data = self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)\n",
    "            \n",
    "        if len(data) > 0:\n",
    "            count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "            self.splitted_to_file(count, data, entity_columns_to_use, process_output_dir)    \n",
    "\n",
    "    def create_datasets_for_count(self, is_SSH=True):\n",
    "        output_process_dir = self._path_dataset_SSH if is_SSH else self._path_dataset_no_SSH\n",
    "        load_message = 'SSH' if is_SSH else 'non-SSH'\n",
    "        \n",
    "        if not exists(output_process_dir):\n",
    "            os.makedirs(output_process_dir)\n",
    "\n",
    "        if is_SSH:\n",
    "            self._CSVManager_erih_meta_with_disciplines = CSVManager(self._path_erih_meta_with_disciplines)\n",
    "        else:\n",
    "            self._set_erih_meta_without_disciplines = CSVManager.load_csv_column_as_set(self._path_erih_meta_without_disciplines, 'id')\n",
    "\n",
    "        data = []\n",
    "        count = 0\n",
    "        for file in tqdm(self._list_coci_files, desc=f'Processing COCI files to build {load_message} files for counting...', total=len(self._list_coci_files), colour='cyan', smoothing=0.1):\n",
    "            chunksize = 10000\n",
    "            with pd.read_csv(file, chunksize=chunksize, sep=\",\") as reader:\n",
    "                for chunk in reader:\n",
    "                    chunk.fillna(\"\", inplace=True)\n",
    "                    df_dict_list = chunk.to_dict(\"records\")\n",
    "                    for line in df_dict_list:\n",
    "                        citing = line.get('citing')\n",
    "                        cited = line.get('cited')\n",
    "\n",
    "                        if is_SSH:\n",
    "                            condition = self._CSVManager_erih_meta_with_disciplines.get_value(citing) or self._CSVManager_erih_meta_with_disciplines.get_value(cited)\n",
    "                        else:\n",
    "                            condition = citing in self._set_erih_meta_without_disciplines and cited in self._set_erih_meta_without_disciplines\n",
    "\n",
    "                        if condition:\n",
    "                            count += 1\n",
    "                            if is_SSH:\n",
    "                                entity_condition1 = self._CSVManager_erih_meta_with_disciplines.get_value(citing)\n",
    "                                entity_condition2 = self._CSVManager_erih_meta_with_disciplines.get_value(cited)\n",
    "                            else:\n",
    "                                entity_condition1 = citing in self._set_erih_meta_without_disciplines\n",
    "                                entity_condition2 = cited in self._set_erih_meta_without_disciplines\n",
    "\n",
    "                            if entity_condition1 and entity_condition2:\n",
    "                                data.append(line)\n",
    "                            elif entity_condition1:\n",
    "                                entity_dict1 = {'citing': citing, 'cited': \"\"}\n",
    "                                data.append(entity_dict1)\n",
    "                            elif entity_condition2:\n",
    "                                entity_dict2 = {'cited': cited, 'citing': \"\"}\n",
    "                                data.append(entity_dict2)\n",
    "\n",
    "                            if int(count) != 0 and int(count) % int(self._interval) == 0:\n",
    "                                data = self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_process_dir)\n",
    "\n",
    "        if len(data) > 0:\n",
    "            count = count + (self._interval - (int(count) % int(self._interval)))\n",
    "            self.splitted_to_file(count, data, self._entity_columns_to_use_q1_q3, output_process_dir)\n",
    "\n",
    "    def count_lines(self, path):\n",
    "        '''This method simply counts and sums the lines of csv files contained in the folder, the path of which is passed as input'''\n",
    "        citations_count = 0\n",
    "        files_list = self.get_all_files(path, '.csv')\n",
    "        for file in tqdm(files_list, total=len(files_list), desc='Counting on files...', colour='red', smoothing=0.1):\n",
    "            results = pd.read_csv(file, sep=\",\")\n",
    "            citations_count += len(results)\n",
    "        return citations_count\n",
    "    \n",
    "    def iterate_erih_meta(self):\n",
    "        ssh_papers = list()\n",
    "        not_ssh_papers = list()\n",
    "        id_disciplines_map = dict()\n",
    "        ssh_disciplines = set()\n",
    "\n",
    "        for filename in tqdm(self._list_erih_meta_files, total=len(self._list_erih_meta_files), desc='Building lists of DOIs over ERIH-PLUS and META...', colour='yellow', smoothing=0.1):\n",
    "            df = pd.read_csv(filename) # it was -> os.path.join(erih_meta_dir_path, filename))\n",
    "            df = df[['id', 'erih_disciplines']] # Attention to the name given to the erih_disciplines column, if erih or ERIH\n",
    "            # fill all the possible NaN or None with \"\"\n",
    "            df = df.fillna('')\n",
    "            # create boolean mask for erih_disciplines column\n",
    "            mask = df['erih_disciplines'] != ''\n",
    "            # filter the dataframe with the above mask\n",
    "            ssh_df = df[mask]\n",
    "            ssh_df = ssh_df.reset_index(drop=True)\n",
    "\n",
    "            for _, row in ssh_df.iterrows():\n",
    "                disciplines = row['erih_disciplines'].split(',')\n",
    "                disciplines = [discipline.strip() for discipline in disciplines]\n",
    "                doi = row['id']\n",
    "                if doi not in id_disciplines_map:\n",
    "                    id_disciplines_map[doi] = disciplines\n",
    "                else:\n",
    "                    id_disciplines_map[doi].extend(disciplines)\n",
    "                for discipline in disciplines:\n",
    "                    if discipline not in ssh_disciplines:\n",
    "                        ssh_disciplines.add(discipline)\n",
    "                    \n",
    "            # Create a second dataframe from the above mask, where are kept only the False rows in the mask\n",
    "            not_ssh_df = df[~mask]\n",
    "            not_ssh_df = not_ssh_df.reset_index(drop=True)\n",
    "            # Get the unique values of the id column\n",
    "            unique_ssh = ssh_df['id'].unique().tolist()\n",
    "            unique_not_ssh = not_ssh_df['id'].unique().tolist()\n",
    "            # Append the unique values to the list\n",
    "            ssh_papers.extend(unique_ssh)\n",
    "            not_ssh_papers.extend(unique_not_ssh)\n",
    "\n",
    "        print('Decoupling DOIs...')\n",
    "        ssh_papers_unique = []\n",
    "        for paper in ssh_papers:\n",
    "            papers = paper.split(' ')\n",
    "            ssh_papers_unique.extend(papers)\n",
    "\n",
    "        not_ssh_papers_unique = []\n",
    "        for paper in not_ssh_papers:\n",
    "            papers = paper.split(' ')\n",
    "            not_ssh_papers_unique.extend(papers)\n",
    "\n",
    "        unique_id_disciplines_map = dict()\n",
    "        for key, value in id_disciplines_map.items():\n",
    "            multiple_keys = key.split(' ')\n",
    "            for k in multiple_keys:\n",
    "                unique_id_disciplines_map[k] = value\n",
    "\n",
    "        print('Creating sets for unique DOIs...')\n",
    "        ssh_set = set(ssh_papers_unique)\n",
    "        not_ssh_set = set(not_ssh_papers_unique)\n",
    "\n",
    "        return ssh_set, not_ssh_set, unique_id_disciplines_map, ssh_disciplines\n",
    "\n",
    "\n",
    "    def execute_count(self, output_dir='OutputFiles/', create_subfiles=False, interval=10000):\n",
    "        if create_subfiles:\n",
    "            self._interval = interval\n",
    "            self._output_dir = output_dir\n",
    "            if not exists(self._output_dir):\n",
    "                os.makedirs(self._output_dir)\n",
    "            self._path_erih_meta_with_disciplines = os.path.join(output_dir, 'erih_meta_with_disciplines')\n",
    "            self._path_dataset_SSH = os.path.join(output_dir, 'dataset_SSH')\n",
    "            self._path_erih_meta_without_disciplines = os.path.join(output_dir, 'erih_meta_without_disciplines')\n",
    "            self._path_dataset_no_SSH = os.path.join(output_dir, 'dataset_no_SSH')\n",
    "\n",
    "        if create_subfiles:\n",
    "            # Answer to question 1\n",
    "            self.create_additional_files(with_disciplines=True)\n",
    "            self.create_datasets_for_count(is_SSH=True)\n",
    "            ssh_citations = self.count_lines(self._path_dataset_SSH)\n",
    "            print('Number of citations that (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta: %d' %ssh_citations)\n",
    "\n",
    "            # Answer to question 3\n",
    "            self.create_additional_files(with_disciplines=False)\n",
    "            self.create_datasets_for_count(is_SSH=False)\n",
    "            not_ssh_citations = self.count_lines(self._path_dataset_no_SSH)\n",
    "            print('Number of citations that (according to COCI) start from and go to publications in OpenCitations Meta that are not included in SSH journals: %d' %not_ssh_citations)\n",
    "        else:\n",
    "            print('\\nSarting the process, be patient, it will take a while...\\n')\n",
    "            \n",
    "            ssh_set, not_ssh_set, id_disciplines_map, ssh_disciplines = self.iterate_erih_meta()\n",
    "\n",
    "            ssh_citations = 0\n",
    "            not_ssh_citations = 0\n",
    "            discipline_counter = {}\n",
    "\n",
    "            for discipline in ssh_disciplines:\n",
    "                discipline_counter[discipline] = {'citing': 0, \n",
    "                                                  'cited': 0}\n",
    "\n",
    "            def count_citations(ssh_set, not_ssh_set, row):\n",
    "                if row['citing'] in ssh_set or row['cited'] in ssh_set:\n",
    "                    return 'ssh'\n",
    "                elif row['citing'] in not_ssh_set and row['cited'] in not_ssh_set:\n",
    "                    return 'not_ssh'\n",
    "                else:\n",
    "                    return 'other'\n",
    "                \n",
    "            def count_disciplines(id_disciplines_map, discipline_counter, row):\n",
    "                if row['citing'] in id_disciplines_map:\n",
    "                    citing_disciplines = id_disciplines_map[row['citing']]\n",
    "                    for discipline in citing_disciplines:\n",
    "                        discipline_counter[discipline]['citing'] += 1\n",
    "                if row['cited'] in id_disciplines_map:\n",
    "                    cited_disciplines = id_disciplines_map[row['cited']]\n",
    "                    for discipline in cited_disciplines:\n",
    "                        discipline_counter[discipline]['cited'] += 1\n",
    "            \n",
    "            def count_citations_in_file(ssh_set, not_ssh_set, id_disciplines_map, discipline_counter, filepath):\n",
    "                df = pd.read_csv(filepath, usecols=['citing', 'cited'])\n",
    "                #citation_counts = df.apply(lambda row: count_citations(ssh_set, not_ssh_set, row), axis=1).value_counts()\n",
    "                count_ssh_citations = 0\n",
    "                count_not_ssh_citations = 0\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    citation_type = count_citations(ssh_set, not_ssh_set, row)\n",
    "                    if citation_type == 'ssh':\n",
    "                        count_ssh_citations += 1\n",
    "                    elif citation_type == 'not_ssh':\n",
    "                        count_not_ssh_citations += 1\n",
    "                    count_disciplines(id_disciplines_map, discipline_counter, row)\n",
    "                \n",
    "                return (count_ssh_citations, count_not_ssh_citations, discipline_counter)\n",
    "                #return (citation_counts.get('ssh', 0), citation_counts.get('not_ssh', 0), discipline_counter)\n",
    "\n",
    "            print('Starting to count...\\n')\n",
    "            start_time = time.time()\n",
    "            if time.time() - start_time > 45:\n",
    "                print('The process is taking a while...')\n",
    "                print('Be aware that the overall speed of the process depends on your machine,')\n",
    "                print('please be patient, a progress bar will appear soon...')\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=self.num_cpu) as executor:\n",
    "                count_citations_partial = partial(count_citations_in_file, ssh_set, not_ssh_set, id_disciplines_map, discipline_counter)\n",
    "                results = list(tqdm(executor.map(count_citations_partial, self._list_coci_files), total=len(self._list_coci_files), desc='Iterating files...', colour='green', smoothing=0.1))\n",
    "\n",
    "            print('Updating results...')\n",
    "            for ssh_count, not_ssh_count, partial_discipline_counter in results:\n",
    "                ssh_citations += ssh_count\n",
    "                not_ssh_citations += not_ssh_count\n",
    "                \n",
    "                for discipline, counts in partial_discipline_counter.items():\n",
    "                    discipline_counter[discipline]['citing'] += counts['citing']\n",
    "                    discipline_counter[discipline]['cited'] += counts['cited']\n",
    "\n",
    "            print('Number of citations that (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta: %d' %ssh_citations)\n",
    "            print('Number of citations that (according to COCI) start from and go to publications in OpenCitations Meta that are not included in SSH journals: %d' %not_ssh_citations)\n",
    "            discipline_citing_more = max(discipline_counter, key=lambda x: discipline_counter[x]['citing'])\n",
    "            print(f\"The discipline with the highest 'citing' count is '{discipline_counter[discipline_citing_more]['citing']}'\")\n",
    "            discipline_more_cited = max(discipline_counter, key=lambda x: discipline_counter[x]['cited'])\n",
    "            print(f\"The discipline with the highest 'cited' count is '{discipline_counter[discipline_more_cited]['cited']}'\")\n",
    "\n",
    "        print('Done...')\n",
    "        return ssh_citations, not_ssh_citations\n",
    "\n",
    "\"\"\"\n",
    "c = Counter(\"/Volumes/Extreme SSD/OS_data/Processed_data/smaller_COCI/\", \"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_Marta/\")\n",
    "count = c.execute_count()\n",
    "print(count)\n",
    "\n",
    "c = Counter(\"/Volumes/Extreme SSD/OS_data/Processed_data/smaller_COCI/\", \"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_Marta/\")\n",
    "\n",
    "#files = c.get_all_files(\"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_Marta/\", '.csv')\n",
    "#print(files)\n",
    "\n",
    "count = c.execute_count(output_dir='/Volumes/Extreme SSD/OS_data/OutputFiles/', create_subfiles=True, interval=10000)\n",
    "print(count)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
