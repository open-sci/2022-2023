{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#D6D58E\">Lorenzo Paolini - OpenScience Project Notebook</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#9FC131\">Research question</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How many citations (according to COCI) involve, either as citing or cited entities, publications in SSH journals (according to ERIH-PLUS) included in OpenCitations Meta? What are the disciplines that cites the most and those cited the most? How many citations start from and go to publications in OpenCitations Meta that are not included in SSH journals?</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#D6D58E\">General abstract - Progressive update</span>\n",
    "#### Last update: Week 1 (20/26 march)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Purpose</b>: we want to find out the following:\n",
    "- by looking at citations data contained in COCI, the number of citations included in Meta which refer to publication in SSH (Social Sciences and Humanities) journals indicated in ERIH-PLUS;\n",
    "- the disciplines citing the most VS the disciplines cited the most;\n",
    "- the citations from/to publication contained in Meta which are not included in SSH journals.\n",
    "\n",
    "We want to create a connection between these three different datasets in order to have an overall view of the citations present in each of them.\n",
    "\n",
    "\n",
    "<b>Methodology</b>: we will approach the problem from a computational point of view, by building a python software able to analyse the data, querying them in order to retrieve the information needed, and to present the results in a clear and understandable way.\n",
    "\n",
    "\n",
    "<b>Findings</b>: for what concerns the findings, up to today, we can't see meaningful differences in the number of citations coming from different disciplines, since it is related to the subject of the study, while the ones cited the most belong to psychology, health and science studies.\n",
    "\n",
    "\n",
    "<b>Originality/Value</b>: our research can be defined as very valuable, since it adds information to existing resources with the aim of facilitating their use and allowing the users to have a clearer view of the data contained in each dataset. Further development will be made. For example, we could analyse other disciplines, to have the same overview as the one created by us but related to other fields.\n",
    "\n",
    "\n",
    "<i><b>Keywords</b>: OpenScience, Citation, OC-COCI, OC-Meta, ERIH-PLUS, journals</i>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 1</span>\n",
    "##### <i>20/03 - 25/03</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week we have defined the abstract for our work. Additionally, I have started to download the data that we will use to carry out our project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 2</span>\n",
    "##### <i>27/03 - 01/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second week, which goes together with the third one (Easter things), I have created my own personal ORCID.</br>\n",
    "Additionally, I have finally downloaded all the data for the final project, and started to explore them in detail. The aim of this exploration was to have a better grasp on what we have at our disposal in order to answer the research questions provided at the top of this notebook.</br>\n",
    "\n",
    "The bigger part of the exploration has been done thanks to pandas and os libraries. I still have some doubts for what concerns COCI in particular. I am not sure about which data should I work on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#D6D58E\">Data Management Plan</span>\n",
    "Together with my group, we have defined the first draft of the data management plan of our project, and deposited it permanently on Zenodo. According to the requests, we have produced it for two datasets:\n",
    "- one for the data we will use for our project, and \n",
    "- another for the software we will develop to analyse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 3</span>\n",
    "##### <i>03/04 - 08/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#D6D58E\">Workflow</span>\n",
    "Together with my group, we have also defined and wrote a first version of our workflow in [protocols.io](https://www.protocols.io/). The workflow is not precisely defined yet, this is due to the fact that we still need to understand better what we aim to do. \n",
    "\n",
    "After an additional review of the workflow, this morning we have obtained a DOI for the first version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 4</span>\n",
    "##### <i>10/04 - 15/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week I have tried to get back the lecture I've missed but I didn't manage to do it all. Nonetheless, I have investigated better the topics about Peer review and did the review to the other group's Data Management Plan, trying to make it as useful as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 5</span>\n",
    "##### <i>17/04 - 22/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed a new release by ERIH-PLUS. We have decided all together to use such version to conduct our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we met several times with the other members of the group in order to revise the DMP and the Protocol according to the reviews we received. New versions of both the research outcomes have been published. Additionally, we discussed and prepared some answers to our reviewers, which will be published and subsequently delivered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another result reahced by means of these meetings has been a first united and commonly agreed version of the final software, which has been reasoned and started to be written. In particular, we decided to re-use part of the code taken from preprocessing operations developed inside OpenCitations, properly linked in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 6</span>\n",
    "##### <i>24/04 - 29/04</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this week, we discussed better the workflow of the project. We have also run some experiments on COCI's preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have started to work in order to provide executable bash files to make the entire process easier to be reproduced. Accordingly, I have created a new branch in our github repository, containing these new files and some tests that needs to be investigated better.\n",
    "\n",
    "For now, I have developed a .sh file useful to automatically download all the original files that will then be processed by an additional .sh file, which has been started to be developed. For now, the preprocessing.sh file contains only a first version of COCI preprocessing operations.\n",
    "\n",
    "I have also written a first version of the README of this new branch, useful to explain how to deal with such files and what commands are needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 7</span>\n",
    "##### <i>01/05 - 06/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewed the original answer to the other group's review of our DMP, according to the double check done with Sara, and send it back to her for publishing it on Zenodo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have started working on the code in order to answer to the three research questions.\n",
    "This work has been done in parallel to the one done by the other members of the group. In this way we should also be able to have different versions capable of solving the problem, but also to double-check with better precision the results that came from our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#9FC131\">Week 8</span>\n",
    "##### <i>08/05 - 13/05</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the codes are now ready. I have produced the answers to both the first and the third research questions, and we are waiting for a double check with the other members' analysis results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the way in which I have thought at the problems, the best way to deal with such big data is to produce smaller versions of the same data, but with less information. According to this, I have stored the DOI's contained in META, that has a SSH publisher in a new .csv file, called ERIH_META. This file has been produced also by the other members of the group but, since it was not properly working on my machine, I devised a way to produce my own copy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Preprocess erih in json\n",
    "erih_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/Processed_ERIH/erih_preprocessed.csv\"\n",
    "\n",
    "erih = pd.read_csv(erih_dir_path, delimiter=';', encoding='utf-8')\n",
    "erih_dict = {}\n",
    "erih_disciplines = set()\n",
    "for idx, row in tqdm(erih.iterrows()):\n",
    "    erih_dict[row[\"venue_id\"]] = []\n",
    "    disciplines = row[\"ERIH_disciplines\"].split(',')\n",
    "    for discipline in disciplines:\n",
    "        erih_dict[row[\"venue_id\"]].append(discipline.strip())\n",
    "        erih_disciplines.add(discipline.strip())\n",
    "\n",
    "with open(\"erih_dict.json\", \"w\") as f:\n",
    "    json.dump(erih_dict, f)\n",
    "\n",
    "with open(\"erih_disciplines.json\", \"w\") as f:\n",
    "    disciplines = {}\n",
    "    for discipline in erih_disciplines:\n",
    "        disciplines[discipline] = 0\n",
    "    json.dump(disciplines, f)\n",
    "\n",
    "# Build erih-meta in csv files\n",
    "\n",
    "meta_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/Processed_META/\"\n",
    "erih_meta_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/ERIH_META_prep/\"\n",
    "meta_filenames = [filename for filename in os.listdir(meta_dir_path) if os.path.isfile(os.path.join(meta_dir_path, filename)) \n",
    "                                                                                and not filename.startswith(\"._\")]\n",
    "\n",
    "for filename in tqdm(meta_filenames):\n",
    "    # read\n",
    "    meta_df = pd.read_csv(os.path.join(meta_dir_path, filename), delimiter=',', encoding='utf-8')\n",
    "    # drop nan from venue column\n",
    "    meta_df = meta_df.dropna(subset=['venue'])\n",
    "    # add a new column\n",
    "    meta_df[\"ERIH_disciplines\"] = \"\"\n",
    "    # iterate over rows\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        # get the venue id\n",
    "        venue_ids = row[\"venue\"].split(' ')\n",
    "        if len(venue_ids) == 1:\n",
    "            venue_id = venue_ids[0]\n",
    "            # check if the venue id is in the erih_dict\n",
    "            if venue_id in erih_dict:\n",
    "                # get the disciplines\n",
    "                disciplines = erih_dict[venue_id]\n",
    "                # append the disciplines to the row\n",
    "                meta_df.at[idx, \"ERIH_disciplines\"] = disciplines\n",
    "        else:\n",
    "            for venue_id in venue_ids:\n",
    "                # check if the venue id is in the erih_dict\n",
    "                if venue_id in erih_dict:\n",
    "                    # get the disciplines\n",
    "                    disciplines = erih_dict[venue_id]\n",
    "                    # append the disciplines to the row\n",
    "                    meta_df.at[idx, \"ERIH_disciplines\"] = disciplines\n",
    "                    break\n",
    "    # save the dataframe -> one by one...\n",
    "    meta_df.to_csv(os.path.join(erih_meta_dir_path, filename), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I have divided each paper in the newly built ERIH-META in order to have a clear view of SSH and NOT_SSH publications. This has been saved in a JSON, as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter each erih_meta csv in order to divide ssh and not_ssh dois in json\n",
    "\n",
    "erih_meta_filenames = [filename for filename in os.listdir(erih_meta_dir_path) if os.path.isfile(os.path.join(erih_meta_dir_path, filename)) \n",
    "                                                                                and not filename.startswith(\"._\")]\n",
    "\n",
    "\n",
    "erih_meta_papers = {'ssh_papers':list(),\n",
    "                    'not_ssh_papers':list()}\n",
    "\n",
    "for filename in tqdm(erih_meta_filenames):\n",
    "    df = pd.read_csv(os.path.join(erih_meta_dir_path, filename))\n",
    "    df = df[['id', 'ERIH_disciplines']]\n",
    "    # fill all the possible NaN or None with \"\"\n",
    "    df = df.fillna('')\n",
    "    # create boolean mask for erih_disciplines column\n",
    "    mask = df['ERIH_disciplines'] != ''\n",
    "\n",
    "    # filter the dataframe with this mask\n",
    "    ssh_df = df[mask]\n",
    "    ssh_df = ssh_df.reset_index(drop=True)\n",
    "\n",
    "    # Create a second dataframe from the above mask, where are kept only the False rows in the mask\n",
    "    not_ssh_df = df[~mask]\n",
    "    not_ssh_df = not_ssh_df.reset_index(drop=True)\n",
    "\n",
    "    # Get the unique values of the id column\n",
    "    unique_ssh = ssh_df['id'].unique().tolist()\n",
    "    unique_not_ssh = not_ssh_df['id'].unique().tolist()\n",
    "\n",
    "    # Append the unique values to the list\n",
    "    erih_meta_papers['ssh_papers'].extend(unique_ssh)\n",
    "    erih_meta_papers['not_ssh_papers'].extend(unique_not_ssh)\n",
    "\n",
    "# Save inside JSON\n",
    "with open(\"erih_meta_papers.json\", \"w\") as f:\n",
    "    json.dump(erih_meta_papers, f)\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above reults, I have looked whether there were (as can be spotted in the .csv of meta) more DOIs for the same work, I have divided them, and then I have removed possible double values, and created a new dictionary of unique DOIs for SSH and NOT_SSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean double dois in erih-meta\n",
    "\n",
    "#load erih json into dict\n",
    "with open(\"erih_meta_papers.json\", \"r\") as f:\n",
    "    erih_meta_papers = json.load(f)\n",
    "\n",
    "ssh_papers = []\n",
    "for ssh_pap in tqdm(erih_meta_papers['ssh_papers']):\n",
    "    papers = ssh_pap.split(' ')\n",
    "    ssh_papers.extend(papers)\n",
    "\n",
    "not_ssh_papers = []\n",
    "for not_ssh_pap in tqdm(erih_meta_papers['not_ssh_papers']):\n",
    "    papers = not_ssh_pap.split(' ')\n",
    "    not_ssh_papers.extend(papers)\n",
    "\n",
    "# Create a new dict with the unique values\n",
    "erih_meta_papers_unique = {'ssh_papers':list(set(ssh_papers)),\n",
    "                            'not_ssh_papers':list(set(not_ssh_papers))}\n",
    "\n",
    "with open(\"erih_meta_papers_unique.json\", \"w\") as f:\n",
    "    json.dump(erih_meta_papers_unique, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I have developed this snippet to answer to both Q1 and Q2 that is still pretty slow. It still needs to be optimized (and to run in parallel cores), I will do it if the answers to the two research questions are correct and if we decide with the other members of the group to use this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "coci_dir_path = \"/Volumes/Extreme SSD/OS_data/Processed_data/smaller_COCI/\"\n",
    "\n",
    "# Scan directory\n",
    "coci_filenames = []\n",
    "with os.scandir(coci_dir_path) as entries:\n",
    "    for entry in tqdm(entries, desc='Iterating filenames...', colour='blue', smoothing=0.1, total=len(os.listdir(coci_dir_path))):\n",
    "        if entry.is_file() and not entry.name.startswith(\"._\"):\n",
    "            coci_filenames.append(entry.name)\n",
    "\n",
    "print('Reading erih-meta...')\n",
    "with open(\"erih_meta_papers_unique.json\", \"r\") as f:\n",
    "    erih_meta_papers_unique = json.load(f)\n",
    "\n",
    "print('Building sets...')\n",
    "ssh_set = set(erih_meta_papers_unique['ssh_papers'])\n",
    "not_ssh_set = set(erih_meta_papers_unique['not_ssh_papers'])\n",
    "\n",
    "ssh_citations = 0\n",
    "not_ssh_citations = 0\n",
    "\n",
    "def count_citations(row):\n",
    "    \"\"\"\n",
    "    This function is used thanks to the apply method of pandas.\n",
    "    \"\"\"\n",
    "    # The row contains an SSH citation? -> This is with an OR\n",
    "    if row['citing'] in ssh_set or row['cited'] in ssh_set:\n",
    "        return 'ssh'\n",
    "    # The row contains a non-SSH citation? -> This is with an AND\n",
    "    elif row['citing'] in not_ssh_set and row['cited'] in not_ssh_set:\n",
    "        return 'not_ssh'\n",
    "    # If not inside\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "for filename in tqdm(coci_filenames, desc='Iterating files...', colour='green', smoothing=0.1):\n",
    "    df = pd.read_csv(os.path.join(coci_dir_path, filename))\n",
    "\n",
    "    # Apply count_citations to each row of the DF\n",
    "    citation_counts = df.apply(count_citations, axis=1).value_counts()\n",
    "    # Increment the SSH citation count and non-SSH citation count\n",
    "    ssh_citations += citation_counts.get('ssh', 0)\n",
    "    not_ssh_citations += citation_counts.get('not_ssh', 0)\n",
    "\n",
    "print(ssh_citations)\n",
    "print(not_ssh_citations)   \n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the results I got from it:\n",
    "- SSH Count in META: 225370804\n",
    "- NOT_SSH Count in META: 985223927"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
